{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import os\n",
    "from Hyperspectral.patchsize import  patch_size\n",
    "from Hyperspectral.trian_models import CNN\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "train_files=8\n",
    "test_files=6\n",
    "batch_size=30"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1. 加载数据**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200, 220, 11, 11) (1, 3200)\n",
      "(2400, 220, 11, 11) (1, 2400)\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    data_path = os.path.join(\"E:\\\\pythonProject\\\\DeepLearning\", r'resources\\data')\n",
    "    train_data=np.array([])\n",
    "    train_label=np.array([])\n",
    "    test_data=np.array([])\n",
    "    test_label=np.array([])\n",
    "\n",
    "    for i in range(train_files):\n",
    "        data_set=sio.loadmat(os.path.join(data_path,\"train_\"+str(patch_size)+\n",
    "                                          '_'+str(i+1)+'.mat'))\n",
    "\n",
    "        if i==0:\n",
    "            train_data=data_set['train_data']\n",
    "            train_label=data_set['train_label']\n",
    "        else:\n",
    "            train_data=np.concatenate((train_data,data_set['train_data']),\n",
    "                                       axis=0)\n",
    "            train_label=np.concatenate((train_label, data_set['train_label']),\n",
    "                                        axis=1)\n",
    "\n",
    "\n",
    "    for i in range(test_files):\n",
    "        data_set=sio.loadmat(os.path.join(data_path,\"test_\"+str(patch_size)+\n",
    "                                          '_'+str(i+1)+'.mat'))\n",
    "\n",
    "        if i==0:\n",
    "            test_data=data_set['test_data']\n",
    "            test_label=data_set['test_label']\n",
    "        else:\n",
    "            test_data=np.concatenate((test_data,data_set['test_data']),\n",
    "                                     axis=0)\n",
    "            test_label=np.concatenate((test_label,data_set['test_label']),\n",
    "                                      axis=1)\n",
    "\n",
    "    train_data=np.transpose(train_data,(0,2,3,1))\n",
    "    train_label=np.squeeze(np.transpose(train_label))\n",
    "    test_data=np.transpose(test_data,(0,2,3,1))\n",
    "    test_label=np.squeeze(np.transpose(test_label))\n",
    "\n",
    "    train_db=tf.data.Dataset.from_tensor_slices((train_data,train_label))\n",
    "    train_db=train_db.shuffle(1000).batch(batch_size)\n",
    "    test_db=tf.data.Dataset.from_tensor_slices((test_data,test_label))\n",
    "    test_db=test_db.shuffle(1000).batch(batch_size)\n",
    "\n",
    "    return train_db,test_db"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2. 训练模型**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cnn_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            multiple                  990500    \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               multiple                  0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch multiple                  2000      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            multiple                  450100    \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               multiple                  0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch multiple                  400       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  80200     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  16884     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  1360      \n",
      "=================================================================\n",
      "Total params: 1,541,444\n",
      "Trainable params: 1,540,244\n",
      "Non-trainable params: 1,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    kernal_size=3\n",
    "    channel1=500\n",
    "    channel2=100\n",
    "\n",
    "    network=CNN(channel1,channel2,kernal_size)\n",
    "    network.build(input_shape=(None,11,11,220))\n",
    "    network.summary()\n",
    "\n",
    "    #%%\n",
    "    lr=0.01\n",
    "    loss_step=0\n",
    "    acc_step=0\n",
    "    optimizer=optimizers.Adam(lr)\n",
    "\n",
    "    total_acc=0\n",
    "    total_num=0\n",
    "\n",
    "    #%%\n",
    "    for epoch in range(1000):\n",
    "        for step,(x,y) in enumerate(train_db):\n",
    "            loss_step+=1\n",
    "            y=tf.one_hot(y,depth=16)\n",
    "            with tf.GradientTape() as tape:\n",
    "                logit = network(x)\n",
    "                logit = tf.nn.softmax(logit,axis=1)\n",
    "                loss = tf.losses.categorical_crossentropy()\n",
    "                loss=tf.reduce_mean(loss,axis=10)\n",
    "\n",
    "            grads=tape.gradient(loss,network.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads,network.trainable_variables))\n",
    "\n",
    "        for step,(x,y) in enumerate(test_db):\n",
    "            acc_step+=1\n",
    "            prob=network(x)\n",
    "            prob=tf.nn.softmax(prob,axis=1)\n",
    "            pre=tf.argmax(prob,axis=1)\n",
    "            correct=tf.equal\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-368948a5",
   "language": "python",
   "display_name": "PyCharm (opencv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}